{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saDBafKhnu2J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61f65f0f-d427-4754-d6c7-73407d877e3b",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/arashnic/book-recommendation-dataset?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24.3M/24.3M [00:01<00:00, 15.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/arashnic/book-recommendation-dataset/versions/3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" drive.mount('/content/drive')\\n! mkdir ~/.kaggle\\n! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\\n! chmod 600 ~/.kaggle/kaggle.json\\n! kaggle datasets download arashnic/book-recommendation-dataset\\n! unzip book-recommendation-dataset.zip \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "!pip install nltk\n",
        "import kagglehub\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "nltk.download('all')\n",
        "path = kagglehub.dataset_download(\"arashnic/book-recommendation-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "''' drive.mount('/content/drive')\n",
        "! mkdir ~/.kaggle\n",
        "! cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download arashnic/book-recommendation-dataset\n",
        "! unzip book-recommendation-dataset.zip '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "books = pd.read_csv(\"/root/.cache/kagglehub/datasets/arashnic/book-recommendation-dataset/versions/3/Books.csv\", low_memory = False)\n",
        "ratings = pd.read_csv(\"/root/.cache/kagglehub/datasets/arashnic/book-recommendation-dataset/versions/3/Ratings.csv\", low_memory = False)\n",
        "users = pd.read_csv(\"/root/.cache/kagglehub/datasets/arashnic/book-recommendation-dataset/versions/3/Users.csv\", low_memory = False)\n",
        "\n",
        "#removing unwanted features\n",
        "books = books[['ISBN', 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher']]\n",
        "#remove null values\n",
        "books.dropna(inplace=True)\n",
        "\n",
        "# Function to clean data (lowercase and remove spaces)\n",
        "def clean_data(x):\n",
        "    if isinstance(x, str):\n",
        "        return str.lower(x.replace(\" \", \"\"))\n",
        "    else:\n",
        "        return ''\n",
        "\n",
        "# Apply data cleaning to the metadata columns\n",
        "features = ['Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher']\n",
        "for feature in features:\n",
        "    books[feature] = books[feature].apply(clean_data)\n",
        "\n",
        "merged = ratings.merge(books, on='ISBN') #Merge Ratings and Books\n",
        "x = merged.groupby('User-ID').count()['Book-Rating'] >= 100 #Filter out users with less than 100 book ratings\n",
        "importantUsers = x[x].index #Get Important users\n",
        "filteredRatings = merged[merged['User-ID'].isin(importantUsers)] #Filter out non important users\n",
        "y = filteredRatings.groupby('Book-Title').count()['Book-Rating'] >= 25 #Filter out books with less that 25 unique ratings\n",
        "importantBooks = y[y].index #Get important books\n",
        "finalRatings = filteredRatings[filteredRatings['Book-Title'].isin(importantBooks)] #Only include important books\n",
        "userItem = finalRatings.pivot_table(index='Book-Title', columns = 'User-ID', values = 'Book-Rating') #Make user Item Matrix\n",
        "userItem.fillna(0, inplace=True) #Fill missing values with 0\n",
        "simScore = cosine_similarity(userItem) #Get similarity Scores"
      ],
      "metadata": {
        "id": "iirXLpKazd-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc0a3764-b26e-4c86-b96a-b4a65dadeb7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-b069815944b5>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  books.dropna(inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contentFiltering(books_df, test_title, top_n=10):\n",
        "    \"\"\" Generates book recommendations based on cosine similarity. \"\"\"\n",
        "    # Clean the input title\n",
        "    title = clean_data(test_title)\n",
        "    # Combine metadata into a single 'soup' column\n",
        "    books_df['soup'] = (\n",
        "        books_df['Book-Title'].fillna('') + ' ' +\n",
        "        books_df['Book-Author'].fillna('') + ' ' +\n",
        "        books_df['Year-Of-Publication'].fillna('').astype(str)\n",
        "    )\n",
        "    # Vectorize the 'soup' column\n",
        "    count_vectorizer = CountVectorizer(stop_words='english', max_features=10000)\n",
        "    count_matrix = count_vectorizer.fit_transform(books_df['soup'])\n",
        "    # Train NearestNeighbors model\n",
        "    nn_model = NearestNeighbors(n_neighbors=top_n + 1, metric='cosine', algorithm='brute')\n",
        "    nn_model.fit(count_matrix)\n",
        "    # Create a reverse mapping of indices and book titles\n",
        "    books_df = books_df.reset_index(drop=True)\n",
        "    indices = pd.Series(books_df.index, index=books_df['Book-Title']).drop_duplicates()\n",
        "    # Check if the title exists in indices\n",
        "    if test_title not in indices:\n",
        "        return f\"Book '{test_title}' not found in the dataset.\"\n",
        "    idx = indices[test_title]\n",
        "    # Find the nearest neighbors\n",
        "    distances, neighbors = nn_model.kneighbors(count_matrix[idx], n_neighbors=top_n + 1)\n",
        "    # Generate recommendations\n",
        "    recommendations = []\n",
        "    for i in range(1, len(neighbors[0])):  # Start from 1 to exclude the input book itself\n",
        "        neighbor_idx = neighbors[0][i]\n",
        "        similarity_score = 1 - distances[0][i]  # Convert distance to similarity\n",
        "        book_title = books_df['Book-Title'].iloc[neighbor_idx]\n",
        "        recommendations.append((book_title, similarity_score))\n",
        "    return recommendations"
      ],
      "metadata": {
        "id": "-tvpExOikX9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_title = \"1984\"\n",
        "recommendations = contentFiltering(books, test_title, top_n=5)\n",
        "\n",
        "# Display recommendations\n",
        "if isinstance(recommendations, list):\n",
        "    print(f\"Books similar to '{test_title}':\")\n",
        "    for book, score in recommendations:\n",
        "        print(f\"{book} (Similarity: {score:.2f})\")\n",
        "else:\n",
        "    print(recommendations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "941juYbVAoJw",
        "outputId": "f4930f64-1a66-4a58-e1d6-222456923bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Books similar to '1984':\n",
            "1984 (Similarity: 0.82)\n",
            "nineteeneighty-four:thefacsimileoftheextantmanuscript (Similarity: 0.82)\n",
            "1984 (Similarity: 0.82)\n",
            "1984(spanishlanguageedition) (Similarity: 0.71)\n",
            "1984 (Similarity: 0.67)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def colabfiltering(book, top_n):\n",
        "    ann_model = NearestNeighbors(n_neighbors=top_n+1, metric='cosine', algorithm='brute')\n",
        "    ann_model.fit(userItem)\n",
        "    book_vector = userItem.loc[book_name].values.reshape(1, -1)\n",
        "    distances, indices = ann_model.kneighbors(book_vector)\n",
        "    recommendations = []\n",
        "    for i in range(1, len(indices[0])):\n",
        "        similar_book = userItem.index[indices[0][i]]\n",
        "        similarity_score = 1 - distances[0][i]\n",
        "        recommendations.append((similar_book, similarity_score))\n",
        "    return recommendations"
      ],
      "metadata": {
        "id": "w5fZYju7rb2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book_name = \"1984\"\n",
        "recommendations = colabfiltering(book_name, 10)\n",
        "\n",
        "if isinstance(recommendations, list):\n",
        "    print(f\"Books similar to '{book_name}':\")\n",
        "    for book, score in recommendations:\n",
        "        print(f\"{book} (Similarity: {score:.2f})\")\n",
        "else:\n",
        "    print(recommendations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "654P8xbRA5Qv",
        "outputId": "7c1ecbdd-98b3-4ee4-dd03-e4b6f4469940",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Books similar to '1984':\n",
            "animalfarm (Similarity: 0.23)\n",
            "lyingawake (Similarity: 0.23)\n",
            "waiting (Similarity: 0.21)\n",
            "bravenewworld (Similarity: 0.21)\n",
            "slaughterhousefiveorthechildren'scrusade:adutydancewithdeath (Similarity: 0.20)\n",
            "therestaurantattheendoftheuniverse(hitchhiker'strilogy(paperback)) (Similarity: 0.19)\n",
            "sarah'swindow (Similarity: 0.19)\n",
            "awakening (Similarity: 0.18)\n",
            "thehandmaid'stale (Similarity: 0.18)\n",
            "rollofthunder,hearmycry (Similarity: 0.18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(scores):\n",
        "    min_score = min(scores.values())\n",
        "    max_score = max(scores.values())\n",
        "    if max_score - min_score == 0:\n",
        "        return {item: 1 for item in scores}\n",
        "    return {item: (score - min_score) / (max_score - min_score) for item, score in scores.items()}"
      ],
      "metadata": {
        "id": "a7pyoAczE2D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hybrid recommendation\n",
        "def hybrid_recommendations(user_id, book_title, content_weight=0.5, collaborative_weight=0.5, num_recommendations=10):\n",
        "    content_recs = contentFiltering(books, book_title, num_recommendations)\n",
        "    collaborative_recs = colabfiltering(user_id, num_recommendations)\n",
        "\n",
        "    combined_scores = {}\n",
        "    for book, score in content_recs:\n",
        "        combined_scores[book] = combined_scores.get(book, 0) + score * content_weight\n",
        "    for book, score in collaborative_recs:\n",
        "        combined_scores[book] = combined_scores.get(book, 0) + score * collaborative_weight\n",
        "\n",
        "    combined_scores = normalize(combined_scores)\n",
        "\n",
        "    sorted_recommendations = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:num_recommendations]\n",
        "    return sorted_recommendations"
      ],
      "metadata": {
        "id": "RQBSRLQBwcAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_user_id = 277427\n",
        "test_book_title = \"1984\"\n",
        "test_content_weight = 0.1\n",
        "test_collaborative_weight = 0.9\n",
        "test_num_recommendations = 20\n",
        "\n",
        "recommendations = hybrid_recommendations(\n",
        "    test_user_id,\n",
        "    test_book_title,\n",
        "    content_weight=test_content_weight,\n",
        "    collaborative_weight=test_collaborative_weight,\n",
        "    num_recommendations=test_num_recommendations\n",
        ")\n",
        "\n",
        "print(\"Hybrid Recommendations:\")\n",
        "for book, score in recommendations:\n",
        "    print(f\"{book} (Hybrid Score: {score:.2f})\")"
      ],
      "metadata": {
        "id": "2ojCu5Kgxb2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01fbb6b3-7b6c-4209-af6f-261ee0ef9acb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid Recommendations:\n",
            "1984 (Hybrid Score: 1.00)\n",
            "animalfarm (Hybrid Score: 0.56)\n",
            "lyingawake (Hybrid Score: 0.30)\n",
            "waiting (Hybrid Score: 0.26)\n",
            "bravenewworld (Hybrid Score: 0.26)\n",
            "slaughterhousefiveorthechildren'scrusade:adutydancewithdeath (Hybrid Score: 0.24)\n",
            "therestaurantattheendoftheuniverse(hitchhiker'strilogy(paperback)) (Hybrid Score: 0.23)\n",
            "sarah'swindow (Hybrid Score: 0.22)\n",
            "awakening (Hybrid Score: 0.21)\n",
            "thehandmaid'stale (Hybrid Score: 0.21)\n",
            "rollofthunder,hearmycry (Hybrid Score: 0.21)\n",
            "thecatcherintherye (Hybrid Score: 0.20)\n",
            "lordoftheflies (Hybrid Score: 0.20)\n",
            "biblioholism:theliteraryaddiction (Hybrid Score: 0.20)\n",
            "perfume:thestoryofamurderer(vintageinternational) (Hybrid Score: 0.20)\n",
            "thevampirelestat(vampirechronicles,bookii) (Hybrid Score: 0.19)\n",
            "timeline (Hybrid Score: 0.19)\n",
            "lookatme (Hybrid Score: 0.18)\n",
            "orangesarenottheonlyfruit (Hybrid Score: 0.18)\n",
            "wordfreak:heartbreak,triumph,genius,andobsessionintheworldofcompetitivescrabbleplayers (Hybrid Score: 0.18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 Evaluation & Experiments\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def compare_baseline(user_item_matrix, test_indices):\n",
        "    global_mean = user_item_matrix.values[user_item_matrix > 0].mean()\n",
        "    user_means = user_item_matrix.mean(axis=1)\n",
        "    item_means = user_item_matrix.mean(axis=0)\n",
        "\n",
        "    actual_ratings = []\n",
        "    global_predictions = []\n",
        "    user_predictions = []\n",
        "    item_predictions = []\n",
        "\n",
        "    for row, col in test_indices:\n",
        "        actual_ratings.append(user_item_matrix.iloc[row, col])\n",
        "        global_predictions.append(global_mean)\n",
        "\n",
        "        # User mean prediction\n",
        "        if user_means.iloc[row] > 0:\n",
        "            user_predictions.append(user_means.iloc[row])\n",
        "        else:\n",
        "            user_predictions.append(global_mean)\n",
        "\n",
        "        # Item mean prediction\n",
        "        if item_means.iloc[col] > 0:\n",
        "            item_predictions.append(item_means.iloc[col])\n",
        "        else:\n",
        "            item_predictions.append(global_mean)\n",
        "        returnVal = {\n",
        "          \"Global Mean RMSE\": np.sqrt(mean_squared_error(actual_ratings, global_predictions)),\n",
        "          \"User Mean RMSE\": np.sqrt(mean_squared_error(actual_ratings, user_predictions)),\n",
        "          \"Item Mean RMSE\": np.sqrt(mean_squared_error(actual_ratings, item_predictions)),\n",
        "        }\n",
        "\n",
        "    return returnVal\n"
      ],
      "metadata": {
        "id": "nm3hW3bRqQqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out books with less than 50 unique ratings\n",
        "filtered_books = filteredRatings.groupby('Book-Title').filter(lambda x: len(x) >= 50)\n",
        "\n",
        "# Ensure test_indices only includes valid row-column indices for the filtered books\n",
        "np.random.seed(42)\n",
        "\n",
        "# Map the filtered books back to user-item matrix indices\n",
        "filtered_indices = [\n",
        "    (row, col)\n",
        "    for row in range(userItem.shape[0])\n",
        "    for col in range(userItem.shape[1])\n",
        "    if userItem.iloc[row, col] > 0 and userItem.columns[col] in filtered_books['Book-Title'].values\n",
        "]\n",
        "\n",
        "# Randomly select indices for the test set from the filtered subset\n",
        "test_sample_size = int(0.02 * len(filtered_indices))\n",
        "test_indices = np.random.choice(range(len(filtered_indices)), size=test_sample_size, replace=False)\n",
        "test_indices = [filtered_indices[idx] for idx in test_indices]\n",
        "\n",
        "# Create a copy of the user-item matrix for training\n",
        "train_data = userItem.copy()\n",
        "\n",
        "# Mask the test set entries in the training data\n",
        "for row, col in test_indices:\n",
        "    train_data.iloc[row, col] = 0  # Set test set entries to 0 in the training data\n"
      ],
      "metadata": {
        "id": "9LM34Jnbqcps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the global mean once\n",
        "global_mean = userItem.values[userItem > 0].mean()\n",
        "\n",
        "# Gather actual ratings for the test set\n",
        "actual_ratings = [userItem.iloc[row, col] for row, col in test_indices]\n",
        "\n",
        "# Evaluate baselines\n",
        "rmse_baseline = compare_baseline(userItem, test_indices)\n",
        "\n",
        "# Print RMSE for baseline methods\n",
        "for baseline, rmse in rmse_baseline.items():\n",
        "    print(f\"{baseline}: {rmse:.4f}\")\n",
        "\n",
        "# Evaluate hybrid recoomendations for top 50 books and RMSE\n",
        "top_fifty = filteredRatings.groupby('Book-Title').count()['Book-Rating'] >= 50 #Filter out books with less that 25 unique ratings\n",
        "hybrid_predictions = []\n",
        "\n",
        "for row, col in test_indices:\n",
        "    user_id = userItem.columns[col]  # Map the column index to the user ID\n",
        "    book_title = userItem.index[row]  # Map the row index to the book title\n",
        "\n",
        "    # Generate hybrid recommendations\n",
        "    hybrid_recommendation_scores = hybrid_recommendations(\n",
        "        user_id=user_id,\n",
        "        book_title=book_title,\n",
        "        content_weight=0.5,  # Adjust weights as needed\n",
        "        collaborative_weight=0.5,  # Adjust weights as needed\n",
        "        num_recommendations=10\n",
        "    )\n",
        "\n",
        "    # Extract the predicted score for the test book or fall back to the global mean\n",
        "    predicted_score = next((score for book, score in hybrid_recommendation_scores if book == book_title), global_mean)\n",
        "    hybrid_predictions.append(predicted_score)\n",
        "\n",
        "# Compute RMSE for hybrid method\n",
        "hybrid_rmse = np.sqrt(mean_squared_error(actual_ratings, hybrid_predictions))\n",
        "print(\"Hybrid Method RMSE:\", hybrid_rmse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "C-deqG_UqdpX",
        "outputId": "13c9ccab-1376-4cfe-b8a5-d315ce85b9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "local variable 'returnVal' referenced before assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-4d644174adf1>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Evaluate baselines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mrmse_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muserItem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Print RMSE for baseline methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-4be5ca1aa6f4>\u001b[0m in \u001b[0;36mcompare_baseline\u001b[0;34m(user_item_matrix, test_indices)\u001b[0m\n\u001b[1;32m     34\u001b[0m         }\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mreturnVal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'returnVal' referenced before assignment"
          ]
        }
      ]
    }
  ]
}